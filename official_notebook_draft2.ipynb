{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77404fbf",
   "metadata": {},
   "source": [
    "<h1 style='fontweight: 6; text-align: center;'>A Multimodal Approach for Resource Allocation During Natural Disasters <br> Part 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b67a9",
   "metadata": {},
   "source": [
    "This notebook details the initial phase of an ongoing project that aims to enhance the usefulness of social media data in natural disaster informatics. It is adapted from the original source code (containing .py files, data files, etc), which will be added to the directory soon. This notebook contains all of the custom code, including the creation of all custom classes and functions, used to manipulate the data and create the machine learning (ML) models.\n",
    "\n",
    "**Note**: The primary source of inspiration for this part of the project was this paper: [A Hybrid Machine Learning Pipeline for Automated Mapping of Events and Locations From Social Media in Disasters](https://ieeexplore.ieee.org/document/8955890). This paper also helped us with the overall structure of the machine learning pipeline. \n",
    "\n",
    "**Note**: if you choose to experiment with this code, you may have to change the directory names to suit your device. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4bfdb8",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "- [Preparation](#preparation)\n",
    "- [Project Classes](#project-classes)\n",
    "  - [Location_Tagger](#location_tagger)\n",
    "  - [Dataset Classes](#dataset-classes)\n",
    "    - [NERDataset](#nerdataset)\n",
    "    - [ClassificationDataset](#classificationdataset)\n",
    "  - [Model Classes](#model-classes)\n",
    "    - [BibertSCV](#bibertscv)\n",
    "  - [CustomLabelEncoder](#customlabelencoder)\n",
    "- [Project Functions](#project-functions)\n",
    "  - [Data Functions](#data-functions)\n",
    "    - [.bio Functions](#bio-functions)\n",
    "    - [Data Cleaning Functions](#data-cleaning-functions)\n",
    "    - [Tokenization](#tokenization)\n",
    "  - [Classification Model Functions](#classification-model-functions)\n",
    "  - [Graphing Functions](#graphing-functions)\n",
    "- [Training the Models](#training-the-models)\n",
    "  - [Location Recognition](#location-recognition)\n",
    "    - [The Data](#the-data)\n",
    "      - [Creating the Tokenizer, Datasets, and DataLoaders](#creating-the-tokenizer-datasets-and-dataloaders)\n",
    "    - [Training and Evaluation](#training-and-evaluation)\n",
    "  - [Classification](#classification)\n",
    "    - [The Data](#the-classification-data)\n",
    "    - [Training](#training)\n",
    "    - [Evaluation on the test set](#evaluation-on-the-test-set)\n",
    "- [Visualization](#visualization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689eb1e1",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE' # temp solution to crashing on my device\n",
    "import geopandas as gpd \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path \n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BertTokenizer, \\\n",
    "AutoModelForTokenClassification\n",
    "import evaluate\n",
    "import copy\n",
    "\n",
    "from pipeline_utils import clean_tweet_dfs\n",
    "\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "from shapely.geometry import Point, Polygon\n",
    "import pyproj\n",
    "from shapely.ops import transform\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (AdamW,RobertaForTokenClassification, AutoConfig, \n",
    "                          AutoModelForTokenClassification)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "from sklearn.utils import column_or_1d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import googlemaps\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import accelerate\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds and the device\n",
    "SEED = 9\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\", torch.cuda.current_device())\n",
    "    print('using cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('using cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a93c3",
   "metadata": {},
   "source": [
    "# Project Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c79c4",
   "metadata": {},
   "source": [
    "This project has several important classes that are necessary for the location recognition and classification pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd1eb2",
   "metadata": {},
   "source": [
    "### Location_Tagger\n",
    "The Location_Tagger class is responsible for labeling tweets with the locations mentioned in the tweets and the geocoordinates most likley connected to those locations. This class works, but the extraction of geocoordinates especially needs some adjustment to make it reliable. Currently, it biases too much to the region the user is interested in (meaning that even if a location mentioned in a tweet is not in the region of interest, the geocoordinate location associated with the tweet may be in the region of interest) and can give faulty locations if the location mentioned in the tweet is unclear or not specific. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039678b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Location_Tagger:\n",
    "    def __init__(self, tweets, model, tokenizer,device,\n",
    "                 loc_labels = ['O', 'I-LOC', 'B-LOC'], non_loc_label = 'O'):\n",
    "        '''\n",
    "        Parameters: \n",
    "        tweets - a list of strings\n",
    "        model - a model (probably one with architecture from the transformers library, like `BertForTokenClassification`) that \n",
    "            outputs indices corresponding with `loc_labels`\n",
    "        tokenizer - the transformers tokenizer to be used with the model\n",
    "        loc_labels - the labels associated with the model output\n",
    "        '''\n",
    "        self.tweets = tweets\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loc_labels = loc_labels\n",
    "        self.device = device\n",
    "        self.non_loc_label = non_loc_label\n",
    "\n",
    "        # variables to hold labeled tweets\n",
    "        self.tags = None \n",
    "        self.filtered = None\n",
    "        self.tweets_with_locs = None\n",
    "        self.combined = False\n",
    "        self.finalized = False\n",
    "\n",
    "        self.gmaps = None\n",
    "        self.ROI = None\n",
    "        self.radius = None\n",
    "        self.region = None\n",
    "        self.strict_bounds = None\n",
    "\n",
    "        self.tweets_with_map_locs = None\n",
    "        self.tweets_with_locs_in_ROI = None\n",
    "\n",
    "    def _tag_tweets_ner(self):\n",
    "        '''\n",
    "        Returns a list of tweets tagged with location labels and \n",
    "        whether they contain a location or not\n",
    "        '''\n",
    "        pipe = pipeline('token-classification', self.model,\n",
    "                        tokenizer=self.tokenizer, device=self.device)\n",
    "        tags = pipe(self.tweets)\n",
    "        for i, li in enumerate(tags):\n",
    "            contains_loc = False\n",
    "            if li: # li means list (list representing the tweet)\n",
    "                for ner_dict in li:\n",
    "                    if ner_dict['entity'] in ['B-LOC', 'I-LOC']:\n",
    "                        contains_loc = True\n",
    "                        break\n",
    "            li.append({'contains_loc': contains_loc})\n",
    "        \n",
    "        self.tags = tags\n",
    "        \n",
    "    \n",
    "    def _filter_tweets(self):\n",
    "        assert self.tags != None, 'self.tags is undefined'\n",
    "\n",
    "        filtered = []\n",
    "        for i, tag in enumerate(self.tags):\n",
    "            if tag[-1]['contains_loc']:\n",
    "                filtered.append((self.tweets[i], self.tags[i]))\n",
    "        \n",
    "        self.filtered = filtered\n",
    "\n",
    "    def _get_locs_from_filtered(self):\n",
    "        '''\n",
    "        Reformats the filtered tweets (tweets with location labels) to\n",
    "        include a list of the locations associated with each tweet \n",
    "        '''\n",
    "        assert self.filtered != None, \"Cannot get locations from self.filtered because self.filtered does not exist.\"\n",
    "        \n",
    "        tweets_with_locs = []\n",
    "        for i, tup in enumerate(self.filtered): # tup = (tweet, dicts)\n",
    "            tweets_with_locs.append((tup[0], {'locations': []}))\n",
    "            for loc_dict in tup[1]:\n",
    "                if 'entity' in loc_dict.keys():\n",
    "                    entity = loc_dict['entity']\n",
    "                    if entity in self.loc_labels and entity != self.non_loc_label:\n",
    "                        tweets_with_locs[i][1]['locations'].append(loc_dict)\n",
    "        \n",
    "        self.tweets_with_locs = tweets_with_locs\n",
    "        \n",
    "    def _combine_locs(self, loc_dicts_list):\n",
    "        '''\n",
    "        For a list (associated with a single tweet) containing \n",
    "        a dictionary per location  \n",
    "        '''\n",
    "        combined_locs = []\n",
    "        current_loc = ''\n",
    "\n",
    "        for i, d in enumerate(loc_dicts_list):\n",
    "            if d['entity'] == 'B-LOC' and current_loc != '':\n",
    "                combined_locs.append(current_loc)\n",
    "                current_loc = d['word']\n",
    "            elif d['entity'] == 'B-LOC':\n",
    "                current_loc = d['word']\n",
    "            elif d['entity'] == 'I-LOC':\n",
    "                current_loc = current_loc + f' {d['word']}'\n",
    "            \n",
    "            if i == len(loc_dicts_list) - 1 and current_loc != '':\n",
    "                combined_locs.append(current_loc)\n",
    "\n",
    "        return combined_locs\n",
    "    \n",
    "    def _combine_locs_in_tweets(self):\n",
    "        tweets_with_locs_combined = []\n",
    "\n",
    "        for i, tweet in enumerate(self.tweets_with_locs):\n",
    "            tweet_txt = self.tweets_with_locs[i][0]\n",
    "            tweets_with_locs_combined.append((tweet_txt, {'locations': []}))\n",
    "\n",
    "            loc_dicts_list = self.tweets_with_locs[i][1]['locations']\n",
    "            \n",
    "\n",
    "            combined_locs = self._combine_locs(loc_dicts_list)\n",
    "\n",
    "            tweets_with_locs_combined[i][1]['locations'] = combined_locs\n",
    "\n",
    "        self.tweets_with_locs = tweets_with_locs_combined\n",
    "        self.combined = True\n",
    "\n",
    "    def _finalize_and_combine_words_in_tweets(self):\n",
    "        assert self.combined, 'location entities in tweets have not been combined yet'\n",
    "\n",
    "        finalized_tweets = []\n",
    "\n",
    "        for tup in self.tweets_with_locs:\n",
    "            tweet = tup[0]\n",
    "            locations = tup[1]['locations']\n",
    "\n",
    "            # will hold the finalized tweet and locations\n",
    "            finalized_tweet = {'tweet': tweet, 'locations': []}\n",
    "\n",
    "            combined_locs = []\n",
    "\n",
    "            for loc in locations:\n",
    "                if combined_locs and loc.startswith('##'):\n",
    "                    # if it's a subword token, append it to the last loc\n",
    "                    combined_locs[-1] += loc\n",
    "\n",
    "                else:\n",
    "                    # start a new loc\n",
    "                    combined_locs.append(loc)\n",
    "            \n",
    "            # join combined locs into a string\n",
    "            for i, loc in enumerate(combined_locs):\n",
    "                if i > 0 and not combined_locs[i].startswith('##'):\n",
    "                    if combined_locs[i-1].startswith('##'):\n",
    "                        combined_locs[i] = combined_locs[i-1] + combined_locs[i]\n",
    "                        combined_locs[i-1] = ''\n",
    "            \n",
    "            finalized_tweet['locations'] = [loc.replace(' ##', '') for loc in combined_locs if loc]\n",
    "\n",
    "            finalized_tweets.append(finalized_tweet)\n",
    "        \n",
    "        self.tweets_with_locs = finalized_tweets\n",
    "        self.finalized = True\n",
    "    \n",
    "    def tag_tweets(self, returns=False):\n",
    "        '''\n",
    "        saves a list of dictionaries where each dictionary contains:\n",
    "            `tweet`: the string representation of the tweet\n",
    "            `locations`: a list of locations recognized in the tweet \n",
    "        '''\n",
    "\n",
    "        self._tag_tweets_ner()\n",
    "        self._filter_tweets()\n",
    "        self._get_locs_from_filtered()\n",
    "        self._combine_locs_in_tweets()\n",
    "        self._finalize_and_combine_words_in_tweets()\n",
    "\n",
    "        if returns:\n",
    "            return self.tweets_with_locs\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def set_gmaps(self, map_client, ROI_coords, radius, region, strict_bounds=True):\n",
    "        self.gmaps = map_client\n",
    "        self.ROI = ROI_coords\n",
    "        self.radius = radius\n",
    "        self.region = region\n",
    "        self.strict_bounds = strict_bounds\n",
    "    \n",
    "    # place is the location name we are looking for\n",
    "    # TODO: make this more robust by not directly biasing toward Houston (in case the location is not actually in Houston)\n",
    "        # NOTE: may need the actual metadata from the tweets, or other context \n",
    "    def _find_first_loc(self, place):\n",
    "        '''\n",
    "    Parameters: \n",
    "        place - the name of the place you are searching for (e.g. 'Central Park', 'roller rink')\n",
    "\n",
    "    Returns:\n",
    "        The first location returned by the googlemaps client in dictionary form with keys (name, address, geometry)\n",
    "\n",
    "        If the api search returns no results, a dictionary of the form for the keys' values (None, None, None) is returned\n",
    "\n",
    "    '''\n",
    "        results = self.gmaps.places(query=place, radius=self.radius, \n",
    "                                    location=self.ROI, region=self.region)\n",
    "        \n",
    "        # get the dictionary within the first list of the main dictionary, where the info for the first location result is kept\n",
    "        if 'results' in results and results['results']:\n",
    "            first_result = results['results'][0]\n",
    "            name = first_result.get('name', None) \n",
    "            geometry = first_result.get('geometry', None)\n",
    "            address = first_result.get('formatted_address', None)\n",
    "\n",
    "            result = {'name': name, 'address': address, 'geometry': geometry}\n",
    "        else:\n",
    "            result = {'name': None, 'address': None, 'geometry': None}\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def _get_gmaps_locs_per_tweet(self):\n",
    "        '''\n",
    "            Creates and saves a dictionary of the form: {'tweet': 'tweet text ...', 'locations': [...]}\n",
    "            where each location is a dictionary containing the name, geometries, and coordinates \n",
    "        '''\n",
    "        tweets_with_map_locs = []\n",
    "        map_locs_idx = 0\n",
    "        for i, tweet_dict in enumerate(self.tweets_with_locs): \n",
    "            locations = tweet_dict['locations'] \n",
    "            for j, location in enumerate(locations): # most tweets in the tweet_dict have only 1 - 2 locations\n",
    "                gloc = self._find_first_loc(place=location) \n",
    "                if not None in gloc.values():\n",
    "                    if j == 0:\n",
    "                        map_locs_idx += 1 # each time a new entry is created in the list, add 1 to keep track of the idx of tweets_with_map_locs\n",
    "                        tweets_with_map_locs.append({'tweet': tweet_dict['tweet'], 'locations': []})\n",
    "                        \n",
    "                    # we subtract 1 b/c otherwise we'd be out of idx bounds (e.g. if we only have 1 sample, the map_locs_idx would be 1 not 0)\n",
    "                    tweets_with_map_locs[map_locs_idx-1]['locations'].append(gloc) \n",
    "        \n",
    "        self.tweets_with_map_locs = tweets_with_map_locs\n",
    "\n",
    "    def _extract_point(client_result):\n",
    "        '''\n",
    "        Parameters:\n",
    "            client result\n",
    "                a dict of the form with keys: name, address, geometry\n",
    "                where geometry includes the 'location' from the maps client's results: {'location': {'lat': ..., 'lng': ...},...}\n",
    "        \n",
    "        Returns:\n",
    "            a shapely.geometry Point object\n",
    "\n",
    "        '''\n",
    "        geometry = client_result['geometry']\n",
    "\n",
    "        location_coords = geometry['location']\n",
    "\n",
    "        lat = location_coords['lat']\n",
    "        lon = location_coords['lng']\n",
    "\n",
    "        point = Point(lon, lat)\n",
    "\n",
    "        return point\n",
    "    \n",
    "    def distance_btw_poly_point(self, poly, point, utm_sys, wgs_sys):\n",
    "        '''\n",
    "        Returns:\n",
    "            distance in meters btw a polygon and a point given in geographic coordinate (wgs) form\n",
    "        '''\n",
    "\n",
    "        wgs = pyproj.CRS(wgs_sys)\n",
    "        utm = pyproj.CRS(utm_sys)\n",
    "\n",
    "        transformer = pyproj.Transformer.from_crs(wgs, utm, always_xy=True).transform\n",
    "\n",
    "        poly_utm = transform(transformer, poly)\n",
    "        point_utm = transform(transformer, point)\n",
    "\n",
    "        distance = point_utm.distance(poly_utm)\n",
    "\n",
    "        return distance\n",
    "    \n",
    "    def _in_region_with_index(self, point, region, using_polygon=False, wgs='EPSG:4326', max_dist=100):\n",
    "        \"\"\"\n",
    "        This function works, but it is still a WIP\n",
    "        Checks if the point is inside the region or within max_dist distance to the region if using polygons.\n",
    "        This version uses a spatial index to speed up the search.\n",
    "        \"\"\"\n",
    "        # Ensure CRS match\n",
    "        if not using_polygon:\n",
    "            assert point.crs == region.crs\n",
    "\n",
    "        if isinstance(point, gpd.GeoDataFrame):  # If point is a GeoDataFrame (single point)\n",
    "            point = point.geometry.iloc[0]  # Get the first geometry (should be a single point)\n",
    "\n",
    "        if using_polygon:\n",
    "            # Directly use `within` for faster operations with polygons\n",
    "            return point.within(region)\n",
    "        else:\n",
    "            # Create spatial index for region\n",
    "            spatial_index = region.sindex\n",
    "\n",
    "            # Ensure it's a Point \n",
    "            if point.geom_type == 'Point':\n",
    "                # Extract x, y coordinates directly from the point (not bounds)\n",
    "                point_coords = point.x, point.y\n",
    "            else:\n",
    "                # if it's not a point, use the bounding box\n",
    "                point_coords = point.bounds  # (minx, miny, maxx, maxy)\n",
    "\n",
    "            # Query the spatial index for potential matching polygons\n",
    "            possible_matches_index = list(spatial_index.intersection(point_coords))\n",
    "\n",
    "            # Now, check distances for the matched polygons\n",
    "            for idx in possible_matches_index:\n",
    "                poly = region.iloc[idx]['geometry']\n",
    "                dist = self._distance_btw_poly_point(poly=poly, point=point, utm_sys='EPSG:32633', wgs_sys=wgs)\n",
    "                if dist <= max_dist:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def _process_tweet(self, tweet_dict, region_of_interest, using_polygon=False):\n",
    "        \"\"\"\n",
    "        Processes a single tweet and checks for locations inside the region of interest.\n",
    "        \"\"\"\n",
    "        locations = tweet_dict['locations']\n",
    "        locs_in_ROI = []\n",
    "\n",
    "        for location in locations:\n",
    "            point = self._extract_point(location)\n",
    "            point_gpd = gpd.GeoDataFrame([{'geometry': point}], crs=region_of_interest.crs).to_crs(region_of_interest.crs)\n",
    "            \n",
    "            # Efficient check if the point is in the region or nearby\n",
    "            if self._in_region_with_index(point=point_gpd, region=region_of_interest, using_polygon=using_polygon):\n",
    "                locs_in_ROI.append(location)\n",
    "\n",
    "        if locs_in_ROI:\n",
    "            return {'tweet': tweet_dict['tweet'], 'locations': locs_in_ROI}\n",
    "        return None\n",
    "\n",
    "    # joblib docs: https://joblib.readthedocs.io/en/latest/parallel.html\n",
    "    def _limit_locs_parallel(self, region_of_interest, using_polygon=False, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Filters the locations within a region of interest (ROI) using parallel processin\n",
    "        where the region of interest is a Geopandas GeoDataFrame\n",
    "\n",
    "        Returns:\n",
    "            a list of dictionaries where each dict has the keys 'tweet' and 'locations' \n",
    "        \"\"\"\n",
    "        region_of_interest = region_of_interest.to_crs(crs=region_of_interest.crs)\n",
    "        \n",
    "        # Use joblib\n",
    "        result = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(self._process_tweet)(tweet_dict, region_of_interest, using_polygon)\n",
    "            for tweet_dict in tqdm(self.tweets_with_map_locs, desc=\"Processing tweets\", total=len(self.tweets_with_map_locs))\n",
    "        )\n",
    "        \n",
    "        # Filter out None results (tweets with no locations in ROI)\n",
    "        self.tweets_with_locs_in_ROI = [tweet for tweet in result if tweet is not None] # b/c if there are no locs in the tweet, process_tweet returns None\n",
    "\n",
    "    \n",
    "    def make_gmaps_locs(self, region_of_interest, using_polygon=False, n_jobs=1):\n",
    "        self._get_gmaps_locs_per_tweet()\n",
    "        self._limit_locs_parallel(region_of_interest, using_polygon, n_jobs)\n",
    "    \n",
    "    def make_tweet_gdf_points(self):\n",
    "        gdf = gpd.GeoDataFrame()\n",
    "        \n",
    "        if self.tweets_with_locs_in_ROI:\n",
    "            for tweet in self.tweets_with_locs_in_ROI: # each tweet represented by a dictionary\n",
    "                locations = tweet['locations']\n",
    "                tag = tweet.get('tag')\n",
    "                label = tweet.get('label')\n",
    "                for location in locations:\n",
    "                    point = self._extract_point(location)\n",
    "                    new_row = pd.DataFrame({'tweet': tweet['tweet'], 'geometry': point, 'tag': tag, 'label': label}, index=[0])\n",
    "                    gdf = gpd.GeoDataFrame(pd.concat([gdf, new_row], ignore_index=True))\n",
    "            \n",
    "            return gdf \n",
    "        else:\n",
    "            for tweet in self.tweets_with_map_locs: # each tweet represented by a dictionary\n",
    "                locations = tweet['locations']\n",
    "                tag = tweet.get('tag')\n",
    "                label = tweet.get('label')\n",
    "                for location in locations:\n",
    "                    point = self._extract_point(location)\n",
    "                    new_row = pd.DataFrame({'tweet': tweet['tweet'], 'geometry': point, 'tag': tag, 'label': label}, index=[0])\n",
    "                    gdf = gpd.GeoDataFrame(pd.concat([gdf, new_row], ignore_index=True))\n",
    "            \n",
    "            return gdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce48cd",
   "metadata": {},
   "source": [
    "### Dataset Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9546900",
   "metadata": {},
   "source": [
    "#### NERDataset\n",
    "This dataset class will be used to hold the cleaned training, testing, and evaluation data for the location recognition segment of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f458cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, tokenized_data, device):\n",
    "        self.device = device\n",
    "        self.tokenized_data=tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        item = {key:torch.tensor(val[idx]).to(self.device) for key, val in self.tokenized_data.items()}\n",
    "\n",
    "        return item "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a5ead",
   "metadata": {},
   "source": [
    "#### ClassificationDataset\n",
    "This dataset class will be used to store the data used for training the classification model, which performs multi-label, multi-class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, target1_encoder, target2_encoder, device):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target1_encoder = target1_encoder\n",
    "        self.target2_encoder = target2_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get raw text and labels\n",
    "        text = self.df.iloc[idx]['raw_words']\n",
    "        target1 = self.df.iloc[idx]['target1']\n",
    "        target2 = self.df.iloc[idx]['target2']\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Get input_ids and attention_mask\n",
    "        input_ids = encoding['input_ids'].squeeze(0).to(device)  # remove the batch dimension\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0).to(device)\n",
    "\n",
    "        # Get sequence length (this is needed for your LSTM)\n",
    "        seq_len = input_ids.size(0)\n",
    "\n",
    "        # Return all necessary items for the model\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'seq_len': seq_len,\n",
    "            'target1': torch.tensor(target1, dtype=torch.long).to(device),  # Ensure it's a tensor of long integers\n",
    "            'target2': torch.tensor(target2, dtype=torch.long).to(device),  # Ensure it's a tensor of long integers\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59667e3d",
   "metadata": {},
   "source": [
    "### Model Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09efc4e",
   "metadata": {},
   "source": [
    "#### BibertSCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09895402",
   "metadata": {},
   "source": [
    "This model is adapted from the paper and code from this project: [Multi-label disaster text classification via supervised contrastive learning for social media data](https://github.com/SCMCmodel/scmc)\n",
    "\n",
    "It uses \"supervised contrastive learning\" to ensure that samples with the same label have similar numerical values in their matrices. This project also adds multi-focal loss to aid with the class imbalances present in the data.\n",
    "\n",
    "**Note**: the NER model uses simple transfer learning from Pytorch's distilbert model, so we do not have a custom class for the NER model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    \"\"\"\n",
    "    Multi-class focal loss implementation\n",
    "    - inputs: raw logits from the model\n",
    "    - targets: true class labels (as integer indices, not one-hot encoded)\n",
    "    \"\"\"\n",
    "    # Convert logits to log probabilities\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Calculate probabilities from log probabilities\n",
    "\n",
    "    # Gather the probabilities corresponding to the correct classes\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)\n",
    "\n",
    "    #  focal adjustment\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BibertSCV(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=2, num_tags=6, hidden_size=256, \n",
    "                 temperature=1, num_layers=1, dropout=0.3, label_class_weights=None, tag_class_weights=None, fl_gamma=2, fl_alpha=1):\n",
    "        super(BibertSCV, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.num_labels = num_labels\n",
    "        self.num_tags = num_tags\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.m = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.critrion_class = nn.MultiLabelSoftMarginLoss()\n",
    "        self.critrion_tag = nn.MultiLabelSoftMarginLoss()\n",
    "        self.ce_class = nn.CrossEntropyLoss(weight=label_class_weights)\n",
    "        self.ce_tag = focal_loss_multiclass\n",
    "        self.ce_tag_alpha=fl_alpha\n",
    "        self.ce_tag_gamma=fl_gamma\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size, hidden_size=hidden_size // 2, num_layers=num_layers, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        self.fc_class = nn.Linear(in_features=hidden_size, out_features=num_tags)\n",
    "        self.fc_tag = nn.Linear(in_features=hidden_size, out_features=num_tags)\n",
    "        self.maxpooling = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, seq_len, target1, target2, flags):\n",
    "        '''\n",
    "        Parameters:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "            seq_len: [batch_size] (sequence lengths for padding handling)\n",
    "            target1: [batch_size] (labels for classification)\n",
    "            target2: [batch_size] (labels for tagging)\n",
    "            flags: 0 means training, 2 means prediction\n",
    "        Return:\n",
    "            logits: torch.Tensor [batch_size, num_labels]\n",
    "        '''\n",
    "        \n",
    "        if hasattr(torch.cuda, 'empty_cache'):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Get the BERT hidden states\n",
    "        input_x = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # # CLS embedding (first token's embedding)\n",
    "        # cls_embed = input_x[:, 0, :]  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Apply dropout\n",
    "        hidden = self.dropout(input_x)\n",
    "\n",
    "        packed_input = rnn_utils.pack_padded_sequence(hidden, seq_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through LSTM layer\n",
    "        packed_outputs, (_, _) = self.lstm(packed_input)\n",
    "    \n",
    "        outputs, _ = rnn_utils.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        # Apply dropout layer\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        \n",
    "        # Summarizes the LSTM output by taking the maximum activation per feature across all tokens.\n",
    "        feat = self.maxpooling(outputs.transpose(1, 2)).squeeze(2)  # [batch_size, hidden_size]\n",
    "\n",
    "        # Classification and Tagging output logits\n",
    "        out_class = self.fc_class(feat)  # logits for class\n",
    "        out_tag = self.fc_tag(feat)      # logits for tagging\n",
    "\n",
    "        if flags == 0:  # Training mode\n",
    "            loss_class_c, loss_tag_c = self.scl(feat, seq_len, target1, target2, flags)\n",
    "            loss_class = self.ce_class(out_class, target1)\n",
    "            loss_tag = self.ce_tag(out_tag, target2, alpha=self.ce_tag_alpha, gamma=self.ce_tag_gamma)\n",
    "\n",
    "            return loss_class_c, loss_tag_c, loss_class, loss_tag\n",
    "\n",
    "        if flags == 2:  # Prediction mode\n",
    "            class_pre = torch.max(out_class, -1)[1]\n",
    "            tag_pre = torch.max(out_tag, -1)[1]\n",
    "\n",
    "            return class_pre, tag_pre, out_class, out_tag \n",
    "\n",
    "    def scl(self, feature, seq_len, target1, target2, flags):\n",
    "        feature_x = self.m(feature) # batch norm\n",
    "        temp_feature = torch.matmul(feature_x, feature.permute(1, 0)) \n",
    "        logit = torch.divide(temp_feature, self.temperature)\n",
    "        loss_class, loss_tag = self.scl_loss(logit, seq_len, target1, target2, flags)\n",
    "        return loss_class, loss_tag\n",
    "    \n",
    "    def scl_loss(self, logit, seq_len, target1, target2, flags):\n",
    "        class_pred = logit\n",
    "        class_true = target1.type_as(class_pred)\n",
    "\n",
    "        tag_pred = logit\n",
    "        tag_true = target2.type_as(tag_pred)\n",
    "\n",
    "        # Similarity calculations for class and tag predictions\n",
    "        # .eq computes element-wise equality\n",
    "        class_true_x = torch.unsqueeze(class_true, -1)\n",
    "        class_true_y = (torch.eq(class_true_x, class_true_x.permute(1, 0))).type_as(class_pred)\n",
    "        class_true_z = class_true_y / torch.sum(class_true_y, 1, keepdim=True)\n",
    "\n",
    "        tag_true_x = torch.unsqueeze(tag_true, -1)\n",
    "        tag_true_y = torch.eq(tag_true_x, tag_true_x.permute(1, 0)).type_as(tag_pred)\n",
    "        tag_true_z = tag_true_y / torch.sum(tag_true_y, 1, keepdim=True)\n",
    "\n",
    "        # Cross entropy loss for class and tag predictions\n",
    "        class_cross_entropy = self.critrion_class(class_pred, class_true_z)\n",
    "        tag_cross_entropy = self.critrion_class(tag_pred, tag_true_z)\n",
    "\n",
    "        return class_cross_entropy, tag_cross_entropy\n",
    "\n",
    "    def predict(self, input_ids, attention_mask, seq_len, target1, target2, flags):\n",
    "        logits = self.forward(input_ids, attention_mask, seq_len, target1, target2, flags)\n",
    "        if self.num_labels > 1:\n",
    "            return torch.argmax(logits, dim=-1)\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6387d",
   "metadata": {},
   "source": [
    "### CustomLabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e0ef3",
   "metadata": {},
   "source": [
    "This label encoder is adjusted from sklearn's LabelEncoder because we want to be able to control the order of the labels (the default LabelEncoder always orders the labels no matter which order the label classes are inputted into the fit method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45395be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLabelEncoder(LabelEncoder):\n",
    "    '''\n",
    "    Modifies the sklearn LabelEncoder to use the inputted labels without sorting them\n",
    "    '''\n",
    "    def fit(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        self.classes_ = pd.Series(y).unique()\n",
    "        return self\n",
    "    def fit_transform(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        self.classes_ = pd.Series(y).unique()\n",
    "        \n",
    "        return [self.classes_.tolist().index(item) for item in y]\n",
    "    def transform(self, y):\n",
    "        return [self.classes_.tolist().index(item) for item in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfbf4c",
   "metadata": {},
   "source": [
    "# Project Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885eea98",
   "metadata": {},
   "source": [
    "### Data Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491b24c",
   "metadata": {},
   "source": [
    "#### .bio Functions\n",
    "These functions are necessary for reading the data from the .bio files that our Twitter data for the location recognition model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d2b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bio_file(f_path):\n",
    "    '''\n",
    "    Reads the .bio file and returns a list of tuples \n",
    "    containing (tokens, labels) pairs, where each tuple\n",
    "    represents the data from a single tweet\n",
    "    '''\n",
    "    data = []\n",
    "    with open(f_path, 'r', encoding='utf-8') as file:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '-DOCSTART- O':\n",
    "                if tokens:\n",
    "                    data.append((tokens, labels))\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            elif line: # ensures we do not use a blank line\n",
    "                token, label = line.split()\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "        if tokens:\n",
    "            data.append((tokens, labels))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# turning the .bio data from the previous function to a DataFrame\n",
    "def bio_to_df(bio_data):\n",
    "    rows = []\n",
    "    for tokens, labels in bio_data:\n",
    "        rows.append({'token': tokens, 'label': labels})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# for converting a list of labels to indices\n",
    "def label_list_as_indices(label_to_idx_dict, a_list):\n",
    "    label_list = []\n",
    "    for token in a_list:\n",
    "        label_list.append(label_to_idx_dict[token])\n",
    "    return label_list\n",
    "\n",
    "# adds a label of indices to the DataFrame\n",
    "def add_df_label_col(df, label_to_idx_dict, label_col):\n",
    "    idx_label_col = df[label_col].apply(lambda x: label_list_as_indices(label_to_idx_dict,x))\n",
    "    new_df = df\n",
    "    new_df['label_idx'] = idx_label_col\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94859f8",
   "metadata": {},
   "source": [
    "#### Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the tweets\n",
    "def clean_tweet_ner(tweet_data):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet_data: (tweet_tokens, token_labels) - corresponds to a single tweet \n",
    "    Returns:\n",
    "        tweet_data: (cleaned_tokens, token_labels) - some tokens (links, punctuation) removed \n",
    "    '''\n",
    "    tokens = tweet_data[0]\n",
    "    labels = tweet_data[1]\n",
    "    new_tokens_labels = ([], [])\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if re.fullmatch(r'https?://[A-Za-z0-9./]+', token, re.IGNORECASE):\n",
    "            continue\n",
    "        elif re.fullmatch(r'^\\s*$', token):\n",
    "            continue\n",
    "        \n",
    "        elif re.fullmatch(r'[^a-zA-Z]', token):\n",
    "            continue\n",
    "            \n",
    "        else: \n",
    "            t = token.strip()\n",
    "            t = re.sub(r'@[A-Za-z0-9]+', '', t)\n",
    "            t = re.sub(r'[^a-zA-Z]', ' ', t) \n",
    "            if t.strip():\n",
    "                new_tokens_labels[0].append(t.strip())\n",
    "                new_tokens_labels[1].append(labels[i])\n",
    "\n",
    "    if new_tokens_labels == ([], []):\n",
    "        return None\n",
    "        \n",
    "    return new_tokens_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107cd2a9",
   "metadata": {},
   "source": [
    "`replace_ner_label` is used to replace one of the location labels (e.g. I-ROAD with I-LOC) with another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ddca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ner_labels(list_of_labels, map_dict):\n",
    "    new_list = [*map(map_dict.get, list_of_labels)]\n",
    "    return new_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929f149",
   "metadata": {},
   "source": [
    "`replace_label` is used when we want to replace one of the disaster labels with another, e.g. placing 'vehicle_damage' under the umbrella category 'other_relevant_information'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a82985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_label(df, label, replacement, label_col):\n",
    "    df[label_col] = df[label_col].apply(lambda x: x if x != label else replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e55c07",
   "metadata": {},
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this function because some words are split into multiple tokens\n",
    "def tokenize_and_align_labels(df, ner_tokenizer):\n",
    "    tokenized_inputs = ner_tokenizer(\n",
    "        df.token.tolist(), truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(df.label_idx.tolist()):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Label only the first token of a word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a77397",
   "metadata": {},
   "source": [
    "### Classification Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_weights(dataset, num_classes):\n",
    "    class_counts = np.zeros(num_classes)\n",
    "    for instance in dataset:\n",
    "        # get the class of the instance, which indexes class_counts\n",
    "        class_counts[instance['target2']] += 1\n",
    "    class_weights = 1.0 / class_counts\n",
    "    class_weights = class_weights / np.sum(class_weights)\n",
    "    return torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_acc(pred_classes, real_classes, pred_tags, real_tags):\n",
    "   \n",
    "    total_count, correct_count = 0.0, 0.0\n",
    "    for p_class, r_class, p_tag, r_tag in zip(pred_classes, real_classes, pred_tags, real_tags):\n",
    "        if p_class == r_class and p_tag == r_tag:\n",
    "            correct_count += 1.0\n",
    "        total_count += 1.0\n",
    "    return 1.0 * correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690066b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev(model, dev_loader):\n",
    "    avg = 'macro'\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss_class = 0\n",
    "    eval_loss_tag = 0\n",
    "    pred_classes = []\n",
    "    true_classes = []\n",
    "    pred_tags = []\n",
    "    true_tags = []\n",
    "\n",
    "    batch_size = 16\n",
    "    with torch.no_grad():\n",
    "        for b, batch in enumerate(dev_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            target1 = batch['target1']\n",
    "            target2 = batch['target2']\n",
    "            seq_len = batch['seq_len']\n",
    "\n",
    "            flag = 0\n",
    "            loss_class_c, loss_tag_c, class_loss, tag_loss = model.forward(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                                                                           seq_len=seq_len, target1=target1, target2=target2, flags=flag)\n",
    "            flag = 2\n",
    "            pred_class, pred_tag, out_class, out_tag = model.forward(input_ids, attention_mask, seq_len, target1, target2, flag)\n",
    "\n",
    "            pred_classes.extend(pred_class.cpu().numpy().tolist())\n",
    "            true_classes.extend(target1.cpu().numpy().tolist())\n",
    "            pred_tags.extend(pred_tag.cpu().numpy().tolist())\n",
    "            true_tags.extend(target2.cpu().numpy().tolist())\n",
    "            eval_loss_class += class_loss.item()\n",
    "            eval_loss_tag += tag_loss.item()\n",
    "\n",
    "            avg_eval_loss_class = eval_loss_class / batch_size\n",
    "            avg_eval_loss_tag = eval_loss_tag / batch_size\n",
    "\n",
    "            label_accuracy = accuracy_score(true_classes, pred_classes)\n",
    "            tag_accuracy = accuracy_score(true_tags, pred_tags)\n",
    "            label_f1 = f1_score(true_classes, pred_classes, zero_division=1, average=avg)\n",
    "            tag_f1 = f1_score(true_tags, pred_tags, zero_division=1, average=avg)\n",
    "            label_precision = precision_score(true_classes, pred_classes, zero_division=1, average=avg)\n",
    "            tag_precision = precision_score(true_tags, pred_tags, zero_division=1, average=avg)\n",
    "            label_recall = recall_score(true_classes, pred_classes, zero_division=1, average=avg)\n",
    "            tag_recall = recall_score(true_tags, pred_tags, zero_division=1, average=avg)\n",
    "\n",
    "            \n",
    "\n",
    "            acc_total = total_acc(pred_classes, true_classes, pred_tags, true_tags)\n",
    "\n",
    "            metrics = {\n",
    "                'label_accuracy': label_accuracy, \n",
    "                'tag_accuracy': tag_accuracy, \n",
    "                'label_f1': label_f1, \n",
    "                'tag_f1': tag_f1, \n",
    "                'label_precision': label_precision,\n",
    "                'tag_precision': tag_precision, \n",
    "                'label_recall': label_recall, \n",
    "                'tag_recall': tag_recall, \n",
    "                'overall_accuracy': acc_total, \n",
    "                'avg_eval_loss_class': avg_eval_loss_class, \n",
    "                'avg_eval_loss_tag': avg_eval_loss_tag\n",
    "            }\n",
    "\n",
    "            wandb.log(metrics)\n",
    "\n",
    "            print('****Evaluation****')\n",
    "            print(f'total_accuracy: {acc_total}')\n",
    "            print(f'label_accuracy: {label_accuracy}')\n",
    "            print(f'tag_accuracy: {tag_accuracy}')\n",
    "            print('******************')\n",
    "\n",
    "            return label_accuracy, tag_accuracy, acc_total\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bibert_pipeline(tweet_dicts, tokenizer, bibert, target1_id2label, target2_id2label, device='cpu', custom_batch_size=16):\n",
    "    labeled_dicts = copy.deepcopy(tweet_dicts)\n",
    "    \n",
    "    batch_size = custom_batch_size\n",
    "    num_full_batches = len(tweet_dicts) // batch_size # if you have 33 tweets, this would be 2\n",
    "    last_batch_len = len(tweet_dicts) % batch_size # if you have 33 tweets, this would be 1\n",
    "\n",
    "    for i in range(num_full_batches):\n",
    "        tweets = [tweet_dict['tweet'] for tweet_dict in tweet_dicts[i*batch_size:i*batch_size+batch_size]]\n",
    "        tokenized_inputs = tokenizer(tweets, padding=True, truncation=True, return_tensors='pt')\n",
    "        tokenized_inputs.to(device)\n",
    "        seq_len = tokenized_inputs['attention_mask'].sum(dim=1).to('cpu')\n",
    "    \n",
    "        informative_pred, tag_pred = bibert.predict(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], seq_len, flags=2)\n",
    "    \n",
    "        informative_pred = informative_pred\n",
    "        tag_pred = tag_pred\n",
    "    \n",
    "        # assert(len(tweets) == len(informative_pred))\n",
    "\n",
    "        for t, tweet in enumerate(tweets): # 16 tweets, each tweet from batch i \n",
    "            loc = t + i*batch_size # this gets the t tweet from the current ith batch # if t is 1, we're on batch 1 so i is 1, then this would be 1 + 16 = 17\n",
    "            labeled_dicts[loc]['label'] = target1_id2label[int(informative_pred[t])]\n",
    "            labeled_dicts[loc]['tag'] = target2_id2label[int(tag_pred[t])]\n",
    "\n",
    "    if last_batch_len > 0:\n",
    "        tweets = [tweet_dict['tweet'] for tweet_dict in tweet_dicts[(num_full_batches-1)*batch_size : (num_full_batches-1)*batch_size + last_batch_len]]\n",
    "        tokenized_inputs = tokenizer(tweets, padding=True, truncation=True, return_tensors='pt')\n",
    "        tokenized_inputs.to(device)\n",
    "        seq_len = tokenized_inputs['attention_mask'].sum(dim=1).to('cpu')\n",
    "    \n",
    "        informative_pred, tag_pred = bibert.predict(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], seq_len, flags=2)\n",
    "    \n",
    "        informative_pred = informative_pred\n",
    "        tag_pred = tag_pred\n",
    "        for t, tweet in enumerate(tweets): # 16 tweets, each tweet from batch i \n",
    "            loc = t + (num_full_batches - 1)*batch_size \n",
    "            labeled_dicts[loc]['label']  = target1_id2label[int(informative_pred[t])]\n",
    "            labeled_dicts[loc]['tag'] = target2_id2label[int(tag_pred[t])]\n",
    "    \n",
    "            \n",
    "    return labeled_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313a160",
   "metadata": {},
   "source": [
    "The collate function below is used as an argument to the DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61999e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    target1 = [item['target1'] for item in batch]\n",
    "    target2 = [item['target2'] for item in batch]\n",
    "    seq_len = [item['seq_len'] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    target1 = torch.stack(target1)\n",
    "    target2 = torch.stack(target2)\n",
    "    seq_len = torch.tensor(seq_len, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids.to(device),\n",
    "        'attention_mask': attention_mask.to(device),\n",
    "        'target1': target1.to(device),\n",
    "        'target2': target2.to(device),\n",
    "        'seq_len': seq_len\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ea5af",
   "metadata": {},
   "source": [
    "### Graphing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34297d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_classes(tweets_gdf, region_gdf, class_to_color, region_color='lightblue', class_col='tag',\n",
    "                       markersize=.7, column=None, cmap=None, legend=False, plot_points=True, fontsize=None, \n",
    "                      fontfam=None, font_color='black', title=None , ticks=True, class_legend=False, linewidth=.5, \n",
    "                      edgecolor='white', axis_off=False, cbar_title = '', vmin=0, vmax=25):\n",
    "    \n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    plt.rcParams['figure.facecolor'] = 'white'\n",
    "    plt.rcParams['legend.facecolor'] = 'white'\n",
    "    \n",
    "    color_gdf = copy.deepcopy(tweets_gdf)\n",
    "    color_gdf.dropna(subset=[class_col], inplace=True)\n",
    "    color_gdf['color'] = color_gdf[class_col].map(class_to_color)\n",
    "    \n",
    "    tweets_gdf.crs=region_gdf.crs\n",
    "    tweets_gdf.to_crs(region_gdf.crs, inplace=True)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    if axis_off:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    base = region_gdf.plot(ax=ax, zorder=1, color=region_color, column=column, cmap=cmap, vmin=vmin, vmax=vmax, edgecolor=edgecolor, linewidth=linewidth) \n",
    "\n",
    "    if legend and column and cmap:\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=mpl.colors.Normalize(vmin=vmin, vmax=vmax))\n",
    "        sm.set_array([])  # Set an empty array for the colorbar\n",
    "        cbar = fig.colorbar(sm, ax=base)\n",
    "        cbar.set_label(cbar_title, fontsize=10)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "        cbar.set_ticks([0, vmax])\n",
    "        cbar.ax.set_aspect(.25)\n",
    "        cbar.outline.set_edgecolor('none')\n",
    "        \n",
    "    if plot_points:\n",
    "        color_gdf.plot(ax=ax, markersize=markersize, c=color_gdf['color'].tolist(), alpha=.85)\n",
    "\n",
    "    if title:\n",
    "        plt.title(label=title, fontsize=12, fontfamily=fontfam, color=font_color)\n",
    "        \n",
    "    if ticks:\n",
    "        plt.xticks(fontsize=fontsize, fontfamily=fontfam, color=font_color)\n",
    "        plt.yticks(fontsize=fontsize, fontfamily=fontfam, color=font_color)\n",
    "\n",
    "    else: \n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "    if class_legend:\n",
    "        handles = [mlines.Line2D([], [], color=class_to_color[cls], label=cls, lw=2, marker='o') for cls in color_gdf[class_col].unique()]\n",
    "        # eight = mlines.Line2D([], [], color='blue', marker='s', ls='', label='8')\n",
    "        # nine = mlines.Line2D([], [], color='blue', marker='D', ls='', label='9')\n",
    "        # etc etc\n",
    "        plt.legend(handles=handles, fontsize=8.7, framealpha=1)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a34d2",
   "metadata": {},
   "source": [
    "# Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d365cf",
   "metadata": {},
   "source": [
    "## Location Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c402b",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0cba9",
   "metadata": {},
   "source": [
    "First, we must have a list of possible location-related labels to be used in natural entity recognition (NER). \n",
    "\n",
    "We start by getting the list of labels that can be associated with a word in our NER data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "os.chdir(home)\n",
    "\n",
    "# can be changed, depending on where your data is being stored\n",
    "label_path = Path('OneDrive - Stephen F. Austin State University', 'Research', \n",
    "                  'Research-Resource-Allocation-Code', 'data', 'HarveyNER-main', \n",
    "                  'data', 'tweets','labels.txt')\n",
    "\n",
    "label_list = []\n",
    "with open(label_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        label_list.append(line)\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dea3c",
   "metadata": {},
   "source": [
    "Using that information, we can refine our list; we do this because we currently only need to know where a location begins and ends in a sentence/tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8de87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# a list of the labels we need\n",
    "simple_label_list = ['B-LOC', 'I-LOC', 'O']\n",
    "# a dictionary assigning each fine-grained label to one of the necessary labels\n",
    "label_to_simple_dict = {'B-POINT':'B-LOC', 'I-AREA':'I-LOC', 'B-AREA':'B-LOC', 'B-RIVER':'B-LOC', \n",
    "                        'I-POINT':'I-LOC', 'I-ROAD':'I-LOC', 'B-ROAD':'B-LOC', 'I-RIVER':'I-LOC', 'O':'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_label_to_idx = {label:idx for idx, label in enumerate(simple_label_list)}\n",
    "simple_idx_to_label = {idx:label for idx, label in enumerate(simple_label_list)}\n",
    "\n",
    "\n",
    "simple_label_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efa579",
   "metadata": {},
   "source": [
    "Importantly, we must ensure that the index labels are always associated with the same label throughout our pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4970fcfd",
   "metadata": {},
   "source": [
    "#### Creating the Tokenizer, Datasets, and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c912e9b",
   "metadata": {},
   "source": [
    "When making the tokenizer, we must create a custom pad token so that the pad_token_id (default is 1) does not interfere with our labels. When we make our model, we will need to increase its embedding size to match the tokenizer's dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d01c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "my_pad = '[pad]'\n",
    "\n",
    "if my_pad not in ner_tokenizer.get_vocab():\n",
    "    ner_tokenizer.add_special_tokens({'pad_token': my_pad})\n",
    "    print('token added')\n",
    "\n",
    "\n",
    "NER_PADDING_TOKEN = ner_tokenizer.convert_tokens_to_ids(my_pad)\n",
    "\n",
    "ner_tokenizer.pad_token_id = NER_PADDING_TOKEN\n",
    "\n",
    "print(f'NER_PADDING_TOKEN: {ner_tokenizer.pad_token_id}')\n",
    "print(f'cls_token: {ner_tokenizer.cls_token_id}')\n",
    "print(f'sep_token: {ner_tokenizer.sep_token_id}')\n",
    "\n",
    "assert(NER_PADDING_TOKEN == ner_tokenizer.pad_token_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8445e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_MAX_LEN = 128 # largest allowable sequence length for this tweet classifier\n",
    "NER_BATCH_SIZE = 32 # needed for the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28de497",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_path = Path('OneDrive - Stephen F. Austin State University', 'Research', 'Research-Resource-Allocation-Code', 'data', 'HarveyNER-main', 'data', 'tweets', 'tweets.train.bio')\n",
    "ner_dev_path = Path('OneDrive - Stephen F. Austin State University', 'Research', 'Research-Resource-Allocation-Code', 'data', 'HarveyNER-main', 'data', 'tweets', 'tweets.dev.bio')\n",
    "ner_test_path = Path('OneDrive - Stephen F. Austin State University', 'Research', 'Research-Resource-Allocation-Code', 'data', 'HarveyNER-main', 'data', 'tweets', 'tweets.test.bio')\n",
    "\n",
    "for path in [ner_train_path, ner_test_path, ner_dev_path]:\n",
    "    assert(path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_data = read_bio_file(ner_train_path)\n",
    "ner_dev_data = read_bio_file(ner_dev_path)\n",
    "ner_test_data = read_bio_file(ner_test_path)\n",
    "print(len(ner_train_data))\n",
    "print(ner_train_data[0])\n",
    "\n",
    "ner_train_data = [clean_tweet_ner(tweet_data) for tweet_data in ner_train_data]\n",
    "ner_dev_data = [clean_tweet_ner(tweet_data) for tweet_data in ner_dev_data]\n",
    "ner_test_data = [clean_tweet_ner(tweet_data) for tweet_data in ner_test_data]\n",
    "print(ner_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra cleaning of the data\n",
    "indices_to_rmv = [] \n",
    "ner_data_lists = [ner_train_data, ner_dev_data, ner_test_data]\n",
    "for ls in ner_data_lists: \n",
    "    indices = []\n",
    "    for i, item in enumerate(ls):\n",
    "        if item==None or item==' ' or item==',':\n",
    "            indices.append(i)\n",
    "    indices_to_rmv.append(indices)\n",
    "\n",
    "for i, index_ls in enumerate(indices_to_rmv):\n",
    "    for idx in sorted(index_ls, reverse=True):\n",
    "        del ner_data_lists[i][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc4970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ner_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9370f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_df = bio_to_df(ner_train_data).dropna(subset=['token', 'label'])\n",
    "ner_dev_df = bio_to_df(ner_dev_data).dropna(subset=['token', 'label'])\n",
    "ner_test_df = bio_to_df(ner_test_data).dropna(subset=['token', 'label'])\n",
    "\n",
    "# adding a label column for the current labels\n",
    "ner_test_df = add_df_label_col(ner_test_df, label_to_idx, 'label')\n",
    "ner_train_df = add_df_label_col(ner_train_df, label_to_idx, 'label')\n",
    "ner_dev_df = add_df_label_col(ner_dev_df, label_to_idx, 'label')\n",
    "\n",
    "simple_ner_train_df = ner_train_df\n",
    "simple_ner_dev_df = ner_dev_df\n",
    "simple_ner_test_df = ner_test_df \n",
    "\n",
    "# replacing the more specific labels with the labels in our simplified label list\n",
    "simple_ner_test_df['label'] = simple_ner_test_df['label'].apply(lambda x: replace_ner_labels(x, label_to_simple_dict))\n",
    "simple_ner_dev_df['label'] = simple_ner_dev_df['label'].apply(lambda x: replace_ner_labels(x, label_to_simple_dict))\n",
    "simple_ner_train_df['label'] = simple_ner_train_df['label'].apply(lambda x: replace_ner_labels(x, label_to_simple_dict))\n",
    "\n",
    "# replace the label column with the labels from our simplified label list\n",
    "simple_ner_train_df = add_df_label_col(simple_ner_train_df, simple_label_to_idx, 'label')\n",
    "simple_ner_dev_df = add_df_label_col(simple_ner_dev_df, simple_label_to_idx, 'label')\n",
    "simple_ner_test_df = add_df_label_col(simple_ner_test_df, simple_label_to_idx, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5056d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ner_test_df.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_tokenized_inputs = tokenize_and_align_labels(simple_ner_train_df, ner_tokenizer)\n",
    "ner_dev_tokenized_inputs = tokenize_and_align_labels(simple_ner_dev_df, ner_tokenizer)\n",
    "ner_test_tokenized_inputs = tokenize_and_align_labels(simple_ner_test_df, ner_tokenizer)\n",
    "ner_test_tokenized_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data into Datasets\n",
    "ner_train_dataset = NerDataset(ner_train_tokenized_inputs, device)\n",
    "ner_dev_dataset = NerDataset(ner_dev_tokenized_inputs, device)\n",
    "ner_test_dataset = NerDataset(ner_test_tokenized_inputs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963adcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(ner_test_dataset)\n",
    "train_dataloader = DataLoader(ner_train_dataset)\n",
    "dev_dataloader = DataLoader(ner_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e671ca",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_DROPOUT = 0\n",
    "\n",
    "num_labels_simple=len(simple_label_list)\n",
    "\n",
    "ner_config = AutoConfig.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "# fewer layers to reduce overfitting\n",
    "ner_config.num_hidden_layers = 3\n",
    "ner_config.num_attention_heads = 3\n",
    "ner_config.num_labels = num_labels_simple\n",
    "ner_config.hidden_dropout_prob = NER_DROPOUT\n",
    "ner_config.id2label = simple_idx_to_label\n",
    "ner_config.label2id = simple_label_to_idx\n",
    "\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'distilbert/distilbert-base-uncased', \n",
    "    config=ner_config)\n",
    "\n",
    "ner_model.to(device)\n",
    "\n",
    "ner_model.resize_token_embeddings(len(ner_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [val for val in simple_idx_to_label.values()] # {'B-LOC': 0, 'I-LOC': 1, 'O': 2}\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ea3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p,l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p,l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        'precision': results['overall_precision'], \n",
    "        'recall': results['overall_recall'],\n",
    "        'f1': results['overall_f1'],\n",
    "        'accuracy': results['overall_accuracy']\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c718d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"5a08d1ebbf0e86ab877a128b98be3c320301b6a0\"\n",
    "wandb.init(project=\"Research-Allocation-During-Disasters\", name=\"ner_location_recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=ner_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daff4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(output_dir='ner_model_out', \n",
    "                                learning_rate=2e-5, \n",
    "                                per_device_train_batch_size=16, \n",
    "                                per_device_eval_batch_size=16, \n",
    "                                num_train_epochs=50, \n",
    "                                weight_decay=.01, \n",
    "                                eval_strategy='epoch', \n",
    "                                save_strategy='epoch', \n",
    "                                load_best_model_at_end=True, \n",
    "                                push_to_hub=False, \n",
    "                                report_to='wandb')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=ner_model, \n",
    "    args=training_args, \n",
    "    train_dataset=ner_train_dataset, \n",
    "    eval_dataset=ner_dev_dataset,\n",
    "    tokenizer=ner_tokenizer, \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(str(Path('ner_model_out', 'best_ner.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9bd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home)\n",
    "\n",
    "ner_model2 = AutoModelForTokenClassification.from_pretrained(Path(home, 'ner_model_out', 'best_ner.pt'))\n",
    "\n",
    "training_args = TrainingArguments(output_dir='ner_model_out', \n",
    "                                learning_rate=2e-5, \n",
    "                                per_device_train_batch_size=16, \n",
    "                                per_device_eval_batch_size=16, \n",
    "                                num_train_epochs=50, \n",
    "                                weight_decay=.01, \n",
    "                                eval_strategy='epoch', \n",
    "                                save_strategy='epoch', \n",
    "                                load_best_model_at_end=True, \n",
    "                                push_to_hub=False, \n",
    "                                report_to='wandb')\n",
    "\n",
    "trainer2 = Trainer(model=ner_model2, \n",
    "    args=training_args, \n",
    "    train_dataset=ner_train_dataset, \n",
    "    eval_dataset=ner_dev_dataset,\n",
    "    tokenizer=ner_tokenizer, \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "\n",
    "trainer2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.evaluate(ner_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b10e4",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0dd2b9",
   "metadata": {},
   "source": [
    "### The Classification Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77963b01",
   "metadata": {},
   "source": [
    "Below, we have a function for preparing and returning the data we use for the classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~')\n",
    "\n",
    "def data_process(device, tar1_labels=None, tar2_labels=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Load the CSV into pandas DataFrame\n",
    "    data_path = Path(home, r'OneDrive - Stephen F. Austin State University\\Research\\Research-Resource-Allocation-Code', \n",
    "                     r'data\\SCMC\\scmc-main\\Data\\CrisisMMD_Multimodal_Crisis_Dataset', 'source', 'nxg')\n",
    "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "    train_df.columns = ['raw_words', 'target1', 'target2']\n",
    "    # train_df['seq_len'] = train_df['raw_words'].apply(lambda seq: len(tokenizer.encode(seq, add_special_tokens=True)))\n",
    "\n",
    "    dev_df = pd.read_csv(os.path.join(data_path, 'dev.csv'))\n",
    "    dev_df.columns = ['raw_words', 'target1', 'target2']\n",
    "    # dev_df['seq_len'] = dev_df['raw_words'].apply(lambda seq: len(tokenizer.encode(seq, add_special_tokens=True)))\n",
    "\n",
    "\n",
    "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "    test_df.columns = ['raw_words', 'target1', 'target2']\n",
    "    # test_df['seq_len'] = test_df['raw_words'].apply(lambda seq: len(tokenizer.encode(seq, add_special_tokens=True)))\n",
    "\n",
    "    \n",
    "    # reducing to 6 classes\n",
    "    df_list = [train_df, dev_df, test_df]\n",
    "    for df in df_list:\n",
    "        replace_label(df, 'vehicle_damage', 'other_relevant_information', 'target2')\n",
    "        replace_label(df, 'missing_or_found_people', 'other_relevant_information', 'target2')\n",
    "    \n",
    "    \n",
    "    target1_encoder = CustomLabelEncoder()\n",
    "    target2_encoder = CustomLabelEncoder()\n",
    "    \n",
    "\n",
    "    # Fit encoders on the labels\n",
    "    if tar1_labels == None and tar2_labels == None:\n",
    "        target1_encoder.fit(train_df['target1'].unique())\n",
    "        target2_encoder.fit(train_df['target2'].unique())\n",
    "    elif tar1_labels == None:\n",
    "        target1_encoder.fit(train_df['target1'].unique())\n",
    "        target2_encoder.fit(tar2_labels)\n",
    "    else:\n",
    "        target1_encoder.fit(tar1_labels)\n",
    "        target2_encoder.fit(tar2_labels)\n",
    "    \n",
    "    # Transform the labels into integer indices\n",
    "    train_df['target1'] = target1_encoder.transform(train_df['target1'])\n",
    "    train_df['target2'] = target2_encoder.transform(train_df['target2'])\n",
    "    dev_df['target1'] = target1_encoder.transform(dev_df['target1'])\n",
    "    dev_df['target2'] = target2_encoder.transform(dev_df['target2'])\n",
    "    test_df['target1'] = target1_encoder.transform(test_df['target1'])\n",
    "    test_df['target2'] = target2_encoder.transform(test_df['target2'])\n",
    "\n",
    "    \n",
    "    # Creating custom datasets\n",
    "    train_dataset = ClassificationDataset(train_df, tokenizer, target1_encoder, target2_encoder, device=device)\n",
    "    dev_dataset = ClassificationDataset(dev_df, tokenizer, target1_encoder, target2_encoder, device=device)\n",
    "    test_dataset = ClassificationDataset(test_df, tokenizer, target1_encoder, target2_encoder, device=device)\n",
    "\n",
    "    return train_dataset, dev_dataset, test_dataset, target1_encoder, target2_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tar1_labels = ['not_informative', 'informative']\n",
    "tar2_labels = ['not_humanitarian', 'other_relevant_information', 'affected_individuals', 'infrastructure_and_utility_damage', 'injured_or_dead_people', 'rescue_volunteering_or_donation_effort']\n",
    "\n",
    "# Load the datasets\n",
    "train_data, dev_data, test_data, tar1_encoder, tar2_encoder = data_process(device=device, tar1_labels=tar1_labels, tar2_labels=tar2_labels)\n",
    "\n",
    "\n",
    "# Initialize tokenizer (using pretrained BERT tokenizer)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tar1_vocab = set([item['target1'] for item in train_data]) \n",
    "tar2_vocab = set([item['target2'] for item in train_data])\n",
    "tar1_vocab_size = len(tar1_vocab)\n",
    "tar2_vocab_size = len(tar2_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar1_labels_id2label = {tar1_encoder.transform([label])[0]: label for label in tar1_labels}\n",
    "tar2_labels_id2label = {tar2_encoder.transform([label])[0]: label for label in tar2_labels}\n",
    "# ensuring that the tar1 label order matches the order in the encoder\n",
    "tar1_labels = tar1_labels_id2label.values()\n",
    "tar2_labels = tar2_labels_id2label.values()\n",
    "\n",
    "print(tar1_labels_id2label)\n",
    "print(tar2_labels_id2label)\n",
    "print(tar1_labels)\n",
    "print(tar2_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65ebb7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55442a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target1_classes = tar1_labels_id2label.keys().sort()\n",
    "target2_classes = tar2_labels_id2label.keys().sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_class_weights = calculate_class_weights(train_data, len(target2_classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model \n",
    "model = BibertSCV(num_labels=len(target1_classes), num_tags=len(target2_classes), \n",
    "                  dropout=.4, fl_gamma=3, fl_alpha=.75, tag_class_weights=tag_class_weights).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [20,40], gamma=0.5, last_epoch=-1)\n",
    "\n",
    "\n",
    "best_label_acc = [0.0, 0.0, 0.0]\n",
    "best_tag_acc = [0.0, 0.0, 0.0]\n",
    "best_total_acc = [0.0, 0.0, 0.0]\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
    "                          collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(home)\n",
    "Path('SCMC_model_fl1/' ).mkdir(parents=True, exist_ok=True)\n",
    "Path('SCMC_model_fl1/' ).mkdir(parents=True, exist_ok=True)\n",
    "Path('SCMC_model_fl1/' ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "epochs = 60\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    step = 0\n",
    "    loss = 0\n",
    "\n",
    "    for b, batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        flag=0\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        target1 = batch['target1']\n",
    "        target2 = batch['target2']\n",
    "        seq_len = batch['seq_len']\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # this seems to work\n",
    "        loss_class_c, loss_tag_c, class_loss, tag_loss = model.forward(input_ids, attention_mask=attention_mask, seq_len=seq_len, target1=target1, \n",
    "                                target2=target2, flags=flag) \n",
    "        loss = 2*(loss_class_c*class_loss) / (loss_class_c + class_loss) + 2*loss_tag_c*tag_loss / (loss_tag_c+tag_loss)\n",
    "        \n",
    "        wandb.log({'train_loss': loss.item()})\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "\n",
    "        if (b+1) % 10 == 0:\n",
    "            # print('loss_csv domain', loss.item())\n",
    "            print(f'batch: {b+1} | loss: {loss.item()}')\n",
    "            \n",
    "\n",
    "    \n",
    "    label_acc, tag_acc, acc_total = dev(model=model, dev_loader=dev_loader)\n",
    "\n",
    "    \n",
    "    if label_acc > best_label_acc[0]:\n",
    "        best_label_acc = [label_acc, tag_acc, acc_total, epoch]\n",
    "        torch.save(model, 'SCMC_model_fl1/' + '_cla.pt')\n",
    "    if tag_acc > best_tag_acc[1]: \n",
    "        torch.save(model, 'SCMC_model_fl1/' + '_tag.pt')\n",
    "        best_tag_acc = [label_acc, tag_acc, acc_total, epoch]\n",
    "    if acc_total > best_total_acc[2]:\n",
    "        torch.save(model, 'SCMC_model_fl1/' + '_total.pt')\n",
    "    \n",
    "    wandb.log({'epoch': epoch})\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca4b73",
   "metadata": {},
   "source": [
    "### Evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c668192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path='SCMC_model_fl1/_tag.pt'):\n",
    "    os.chdir(home)\n",
    "    model_testset = torch.load(model_path)\n",
    "    # we already made the test_loader\n",
    "    avg = 'macro'\n",
    "\n",
    "    test_loss_class = 0\n",
    "    test_loss_tag = 0\n",
    "    pred_classes = []\n",
    "    true_classes = []\n",
    "    pred_tags = []\n",
    "    true_tags = []\n",
    "    \n",
    "    flag = 2\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, batch in enumerate(test_loader):\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            target1 = batch['target1']\n",
    "            target2 = batch['target2']\n",
    "            seq_len = batch['seq_len']\n",
    "\n",
    "            flag = 2\n",
    "            pred_class, pred_tag, out_class, out_tag = model.forward(input_ids, attention_mask, seq_len, target1, target2, flag)\n",
    "\n",
    "            pred_classes.extend(pred_class.cpu().numpy().tolist())\n",
    "            true_classes.extend(target1.cpu().numpy().tolist())\n",
    "            pred_tags.extend(pred_tag.cpu().numpy().tolist())\n",
    "            true_tags.extend(target2.cpu().numpy().tolist())\n",
    "            test_loss_class += class_loss.item()\n",
    "            test_loss_tag += tag_loss.item()\n",
    "\n",
    "            avg_test_loss_class = test_loss_class / batch_size\n",
    "            avg_test_loss_tag = test_loss_tag / batch_size\n",
    "\n",
    "            label_accuracy = accuracy_score(true_classes, pred_classes)\n",
    "            tag_accuracy = accuracy_score(true_tags, pred_tags)\n",
    "            label_f1 = f1_score(true_classes, pred_classes, zero_division=1, average=avg)\n",
    "            tag_f1 = f1_score(true_tags, pred_tags, zero_division=1, average=avg)\n",
    "            label_precision = precision_score(true_classes, pred_classes, zero_division=1, average=avg)\n",
    "            tag_precision = precision_score(true_tags, pred_tags, zero_division=1, average=avg)\n",
    "            label_recall = recall_score(true_classes, pred_classes, zero_division=1, average=avg)\n",
    "            tag_recall = recall_score(true_tags, pred_tags, zero_division=1, average=avg)\n",
    "\n",
    "            \n",
    "\n",
    "            acc_total = total_acc(pred_classes, true_classes, pred_tags, true_tags)\n",
    "\n",
    "            metrics = {\n",
    "                'test_label_accuracy': label_accuracy, \n",
    "                'test_tag_accuracy': tag_accuracy, \n",
    "                'test_label_f1': label_f1, \n",
    "                'test_tag_f1': tag_f1, \n",
    "                'test_label_precision': label_precision,\n",
    "                'test_tag_precision': tag_precision, \n",
    "                'test_label_recall': label_recall, \n",
    "                'test_tag_recall': tag_recall, \n",
    "                'test_overall_accuracy': acc_total, \n",
    "                'test_avg_loss_class': avg_test_loss_class, \n",
    "                'test_avg_loss_tag': avg_test_loss_tag\n",
    "            }\n",
    "\n",
    "            wandb.log(metrics)\n",
    "\n",
    "            print('****Evaluation on SCMC CrisisMMD Test Set****')\n",
    "            print(f'total_accuracy: {acc_total}')\n",
    "            print(f'label_accuracy: {label_accuracy}')\n",
    "            print(f'tag_accuracy: {tag_accuracy}')\n",
    "            print('******************')\n",
    "\n",
    "            return label_accuracy, tag_accuracy, acc_total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_accuracy, test_tag_accuracy, test_acc_total = test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23e824",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b13e9",
   "metadata": {},
   "source": [
    "### Tag the tweets with locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model checkpoint\n",
    "checkpoint = 'best_ner.pt'\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(Path(home, 'ner_model_out', checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397787cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_dir = 'OneDrive - Stephen F. Austin State University/Research/Research-Resource-Allocation-Code' \n",
    "\n",
    "all_together_csv = Path(home, research_dir, 'data', 'Hurricane_Harvey_Tweets_for_Graphing', 'Hurricane_Harvey_utf8.csv') # unlabeled, raw \n",
    "# creating a df from the unlabeled, raw tweet data\n",
    "raw_tweets_df = pd.read_csv(all_together_csv)\n",
    "raw_tweets_df = raw_tweets_df.reset_index(drop=True)\n",
    "\n",
    "raw_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_df = clean_tweet_dfs([raw_tweets_df], 'Tweet')[0][0]\n",
    "len(clean_tweets_df)\n",
    "clean_tweets_list = clean_tweets_df['Tweet'].tolist()\n",
    "clean_tweets_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b40265",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_df = clean_tweet_dfs([raw_tweets_df], 'Tweet')[0][0]\n",
    "clean_tweets_df.drop_duplicates(subset=['Tweet'], inplace=True) # idk why this data contains so many duplicates, but it does, so this is necessary\n",
    "clean_tweets_df = clean_tweets_df\n",
    "\n",
    "clean_tweets_list = clean_tweets_df['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Location_Tagger(clean_tweets_list, ner_model, ner_tokenizer, device)\n",
    "\n",
    "tagged = tagger.tag_tweets(returns=True)\n",
    "\n",
    "tagged[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da9d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default geocoding API and API parameters for getting the google maps locations per tweet\n",
    "key = ...\n",
    "gmaps = googlemaps.Client(key=key)\n",
    "houston_ROI = (29.7604, -95.3698)\n",
    "houston_radius = 20000 # I think this is about how large the Houston area is, but I will be limiting the locs to harris county anyway. I should still check this though\n",
    "region = 'us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.set_gmaps(gmaps, houston_ROI, houston_radius, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f023605",
   "metadata": {},
   "outputs": [],
   "source": [
    "harris_county_census_tracts_path = str(Path(home, 'OneDrive - Stephen F. Austin State University/Research/Research-Resource-Allocation-Code/data/geodata/2020_Harris_County_Census_Tracts/2020_Harris_County_Census_Tracts.shp'))\n",
    "\n",
    "harris_county_census_tracts_gdf = gpd.read_file(harris_county_census_tracts_path)\n",
    "\n",
    "harris_county_census_tracts_gdf.to_crs(epsg=4326, inplace=True) # important \n",
    "harris_county_census_tracts_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.make_gmaps_locs(harris_county_census_tracts_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d64d95",
   "metadata": {},
   "source": [
    "### Tag the tweets with humanitarian classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dicts = tagger.tweets_with_locs_in_ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('SCMC_model_fl1/' + '_tag.pt')\n",
    "bibert = BibertSCV.load_state_dict(state_dict=state_dict)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda76f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dicts = bibert_pipeline(tweet_dicts, tokenizer, bibert, \n",
    "                                tar1_labels_id2label, tar2_labels_id2label, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a0d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.tweets_with_locs_in_ROI = labeled_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52825e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = tagger.make_tweet_gdf_points()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a1fd0",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f224e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the parameters to improve the appearance or change the class that you are graphing\n",
    "graph_classes(tweets_gdf=gdf, region_gdf=harris_county_census_tracts_gdf, class_to_color='informative', \n",
    "              class_col='label', ticks=False, plot_points=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
